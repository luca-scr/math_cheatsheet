% mode: Tex-Pdf -*-
\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage[bookmarks=TRUE,
            colorlinks,
            pdfpagemode=none,
            pdfstartview=FitH,
            citecolor=black,
            filecolor=black,
            linkcolor=black,
            urlcolor=black,
            ]{hyperref}
\usepackage{graphicx}
\usepackage{icomma}
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\input{isomath}

\newcommand{\R}{\texttt{R}\xspace}

\title{Simulated Maximum Likelihood}
\author{Ott Toomet}

\begin{document}
<<setup, include=FALSE>>=
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(warning=FALSE, error=TRUE, message=FALSE, echo=FALSE,
                      cache=TRUE,
                      fig.height=140/25.4)
doParallel::registerDoParallel(parallel::detectCores()/2)
library(dplyr)
library(magrittr)
library(doParallel)
library(ggplot2)
set.seed(1)
@ 

\maketitle

\section{Setup}
\label{sec:setup}

Let's model the data by Poisson distribution.  The data ($N$)
observations are labeled $n_{1}, n_{2}, \dots, n_{N}$.  The task is to
find the Poisson parameter $\lambda$.

We implement the SML as follows: randomly simulate $R$ Poisson$(\lambda)$
numbers $\check n_{1}, \check n_{2}, \dots, \check n_{R}$.  These
number give us simulated probability distribution $\check\Pr(n;
\lambda, R)$.  The
SML parameter estimate is computed as
\begin{equation}
  \label{eq:sml_parameter}
  \check \lambda = \arg\max_{\lambda} 
  \sum_{i=1}^{N} \log \check \Pr(n_{i}; \lambda, R)
\end{equation}
where $n_{i}$ are the data.  Alternatively, $\check\lambda$ is defined
as the root of the score equation:
\begin{equation}
  \label{eq:sml_score}
  \check g(\check\lambda) 
  \equiv
  \pderiv{\lambda} 
  \sum_{i=1}^{N} \log \check \Pr(n_{i}; \lambda, R)
  \left|_{\lambda = \check\lambda}\right.
  = 0.
\end{equation}
What is $\E_{R}\check\lambda$, the expected value over trials, and how is it related to the true value $\lambda^{*}$?

The simulated probability $\check \Pr(n; \lambda, R) = \#(n)/R$
where $\#(n)$ is the number of times, out of $R$, the simulation
yielded $n$.  Hence $R \cdot\check \Pr(n; \lambda, R)$ are the binomial
counts, out of $R$ trials, with probability equal
to the Poisson probability $P^{P}(n; \lambda) = \lambda^{n}/n!
\cdot\me^{-\lambda}$.  Hence we can write it's distribution as
\begin{equation}
  \label{eq:binomial_simulated_poisson}
  \Pr \left[
    \check \Pr(n; \lambda, R)
    =
    \frac{m}{R} \right]
  =
  C_{m}^{R}
  \left[P^{P}(n;\lambda) \right]^{m} 
  \left[1 - P^{P}(n;\lambda)^{m} \right]^{R-m}.
\end{equation}
By the property of binomial distribution, it's expectation is the
Poisson probability
\begin{equation}
  \label{eq:EcheckPr}
  \E \check \Pr(n; \lambda, R) =
  \sum_{m=0}^{R}   
  \frac{m}{R} \cdot
  C_{m}^{R}
  \left[P^{P}(n;\lambda) \right]^{m} 
  \left[1 - P^{P}(n;\lambda)^{m} \right]^{R-m}
  =
  P^{P}(n;\lambda).
\end{equation}
This is just a re-statement that we are using unbiased DGP.  

Expected value of simulated log-likelihood, however is
$\E \log \check \Pr(n; \lambda, R)$.  This number, obviously, does not
exist for any finite $R$ as $\Pr [ \check \Pr(n; \lambda, R) = 0]$ is
always positive, so we replace it by $\log \check \Pr(n; \lambda, R) +
\kappa$ where $\kappa$ is a small number (we use the value $10^{-9}$
below). 

Expand $\log\check \Pr(n; \lambda, R)$ into the Taylor's series around
$P^{P}(n;\lambda)$:
\begin{equation}
  \label{eq:log_taylor}
   \log x 
    \log x_{0} +
    \frac{1}{x_{0}} (x - x_{0}) -
    \frac{1}{2x_{0}^{2}} (x - x_{0})^{2} +
    \frac{1}{3x_{0}^{3}} (x - x_{0})^{3} -
    \dots
\end{equation}
How precise are these approximations?

<<logTaylor>>=
logt <- function(x0, r) {
   if(r <= 0) {
      function(x) rep(log(x0), length(x))
   }
   else {
      function(x) logt(x0, r - 1)(x) - (-1/x0)^r*(x - x0)^r/r
   }
}
x <- seq(0.01, 1, length=100)
x0 <- 0.2
                           # approximate around probability 0.2
curves <- data.frame(r="log", x, approx=log(x), stringsAsFactors=FALSE)
for(r in 1:9) {
   curves <- rbind(curves, data.frame(r, x, approx=logt(x0, r)(x), stringsAsFactors=FALSE))
}
ggplot(curves, aes(x, approx)) + 
   geom_line(aes(col=r)) +
   coord_cartesian(ylim=c(-3,0))
@ 
Convergence appears to be slow.



\section{Data}
\label{sec:data}

We use Poisson distribution with true parameter value $5$.  

\section{Analyze the Bias as $N\to\infty$}
\label{sec:bias}

<<poisData>>=
Rs <- c(3,10,30,300,1000)
eps <- 1e-9
                           # to avid -Inf in logarithm
@ 

SML estimator has a bias term proportional to $\sqrt{N}/R$.  Let's
analyze how do the curves look like for $R = \{\Sexpr{Rs}\}$ and $N =
\{10, 10,000, 10,000,000\}$.

Start with 10 observations.  Let's simulate the estimator on grid with
different $R$:

<<poisGrid, depends="poisData">>=
N <- 10
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=150)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title= paste0("N=", N),
        subtitle=paste0("true value ", mean(x))
        )
@ 

No obvious bias is visible for low-$R$ estimators.  We can see,
though, that anything less than $R=300$ is very noisy.

Let's increase the sample size to 10000 and repeat:

<<poisLargeData, depends="poisData">>=
N <- 10000
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=150)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 

The true values is \Sexpr{mean(x)}.  We can see downward bias for the
smallest $R$, otherwise it is more upward than downward bias.  The
curves are equally noisy.

Now let's do the experiment with 10M observations:

<<pois1e7, depends="poisData">>=
N <- 1e7
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=150)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 
I don't see any major difference between 10M and 10k observations.
The bias is there for small $R$ but it appears to be of comparable
size. 


\subsection{Analyze the Chatter}
\label{sec:chatter}

If $R$ is fixed, the SLL curves are not continuous even if
$N\to\infty$.  Start with $N=10$:

<<poisFixedSeed, depends="poisData">>=
N <- 10
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=250)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      set.seed(5)
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 
The results are clearly less noisy, but only piecewise continuous.
All estimates seem to have downward bias.  Repeat it with $n=10000$:

<<poisFixedSeed10k, depends="poisData">>=
N <- 10000
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=250)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      set.seed(5)
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 
The curves are noticeably smoother, and downward bias is easily
visible for lower $R$-s.  Again, the curves are only piecewise
continuous.  I fail to see a substantial improvement over the case
with chatter above, although the curves are smoother.


10M observations:

<<poisFixedSeed10M, depends="poisData">>=
N <- 1e7
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=250)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      set.seed(5)
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 

\end{document}
