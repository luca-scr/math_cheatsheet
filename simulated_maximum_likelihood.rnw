% mode: Tex-Pdf -*-
\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage[bookmarks=TRUE,
            colorlinks,
            pdfpagemode=none,
            pdfstartview=FitH,
            citecolor=black,
            filecolor=black,
            linkcolor=black,
            urlcolor=black,
            ]{hyperref}
\usepackage{graphicx}
\usepackage{icomma}
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\input{isomath}

\newcommand{\R}{\texttt{R}\xspace}

\title{Simulated Maximum Likelihood}
\author{Ott Toomet}

\begin{document}
<<setup, include=FALSE>>=
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(warning=FALSE, error=TRUE, message=FALSE, echo=FALSE,
                      cache=TRUE,
                      fig.height=140/25.4)
doParallel::registerDoParallel(parallel::detectCores()/2)
library(dplyr)
library(magrittr)
library(doParallel)
library(ggplot2)
set.seed(1)
source("simulated_maximum_likelihood.R")
@ 

\maketitle

\section{Setup}
\label{sec:setup}

Let's model the data by Poisson distribution.  The data ($N$)
observations are labeled $n_{1}, n_{2}, \dots, n_{N}$.  The task is to
find the Poisson parameter $\lambda$.

We implement the SML as follows: randomly simulate $R$ Poisson$(\lambda)$
numbers $\check n_{1}, \check n_{2}, \dots, \check n_{R}$.  These
number give us simulated probability distribution $\check\Pr(n;
\lambda, R)$.  The
SML parameter estimate is computed as
\begin{equation}
  \label{eq:sml_parameter}
  \check \lambda = \arg\max_{\lambda} 
  \sum_{i=1}^{N} \log \check \Pr(n_{i}; \lambda, R)
\end{equation}
where $n_{i}$ are the data.  Alternatively, $\check\lambda$ is defined
as the root of the score equation:
\begin{equation}
  \label{eq:sml_score}
  \check g(\check\lambda) 
  \equiv
  \pderiv{\lambda} 
  \sum_{i=1}^{N} \log \check \Pr(n_{i}; \lambda, R)
  \left|_{\lambda = \check\lambda}\right.
  = 0.
\end{equation}
What is $\E_{R}\check\lambda$, the expected value over trials, and how is it related to the true value $\lambda^{*}$?

The simulated probability $\check \Pr(n; \lambda, R) = \#(n)/R$
where $\#(n)$ is the number of times, out of $R$, the simulation
yielded $n$.  Hence $R \cdot\check \Pr(n; \lambda, R)$ are the binomial
counts, out of $R$ trials, with probability equal
to the Poisson probability $P^{P}(n; \lambda) = \lambda^{n}/n!
\cdot\me^{-\lambda}$.  Hence we can write it's distribution as
\begin{equation}
  \label{eq:binomial_simulated_poisson}
  \Pr \left[
    \check \Pr(n; \lambda, R)
    =
    \frac{m}{R} \right]
  =
  C_{m}^{R}
  \left[P^{P}(n;\lambda) \right]^{m} 
  \left[1 - P^{P}(n;\lambda)^{m} \right]^{R-m}.
\end{equation}
By the property of binomial distribution, it's expectation is the
Poisson probability
\begin{equation}
  \label{eq:EcheckPr}
  \E \check \Pr(n; \lambda, R) =
  \sum_{m=0}^{R}   
  \frac{m}{R} \cdot
  C_{m}^{R}
  \left[P^{P}(n;\lambda) \right]^{m} 
  \left[1 - P^{P}(n;\lambda)^{m} \right]^{R-m}
  =
  P^{P}(n;\lambda).
\end{equation}
This is just a re-statement that we are using unbiased DGP.  It's
variance, accordingly, is $\var \check \Pr(n; \lambda, R) =
P^{P}(n;\lambda)[1 - P^{P}(n;\lambda)]/R$.

Expected value of simulated log-likelihood, however is
$\E \log \check \Pr(n; \lambda, R)$.  This number, obviously, does not
exist for any finite $R$ as $\Pr [ \check \Pr(n; \lambda, R) = 0]$ is
always positive, so we replace it by $\log [\check \Pr(n; \lambda, R) +
\kappa]$ where $\kappa$ is a small positive number (we use the value $10^{-9}$
below). 

Expand $\log[\check \Pr(n; \lambda, R) + \kappa]$ into the Taylor's series around
$P^{P}(n;\lambda)$:
\begin{align*}
%  \label{eq:log_taylor}
  \log[ \check \Pr(n; \lambda, R) +\kappa]
  & =
    \log P^{P}(n;\lambda) +\\
  & + \frac{1}{P^{P}(n;\lambda)} (\check \Pr(n; \lambda, R) -
    P^{P}(n;\lambda) + \kappa) -
  \\
  & - \frac{1}{2 P^{P}(n;\lambda)^{2}} (\check \Pr(n; \lambda, R) -
    P^{P}(n;\lambda) + \kappa)^{2} +
    \\
  & + \frac{1}{3 P^{P}(n;\lambda)^{3}} (\check \Pr(n; \lambda, R) -
    P^{P}(n;\lambda) + \kappa)^{3} -
    \dots
\end{align*}
How precise are these approximations?  Approximate around $x_{0} =
0.5$.  

<<logTaylor>>=
logt <- function(x0, r, kappa=1e-9) {
   if(r <= 0) {
      function(x) rep(log(x0), length(x))
   }
   else {
      function(x) logt(x0, r - 1, kappa)(x) - (-1/x0)^r*(x - x0 + kappa)^r/r
   }
}
x <- seq(0.01, 1, length=100)
x0 <- 0.5
                           # approximate around probability 0.5
kappa <- 1e-9
curves <- data.frame(r="log", x, approx=log(x + kappa), stringsAsFactors=FALSE)
for(r in 1:9) {
   curves <- rbind(curves, 
                   data.frame(r, x, approx=logt(x0, r, kappa)(x), stringsAsFactors=FALSE))
}
ggplot(curves, aes(x, approx)) + 
   geom_line(aes(col=r)) +
   coord_cartesian(ylim=c(-3,0))
@ 
Seems like 10 terms give quite a good approximation, except in the
far left tail.

For $\E \log [\check \Pr(n; \lambda, R) = \kappa]$ we are looking for the expectation
\begin{equation}
  \label{eq:taylor}
  \begin{split}
  \E [ \log \check \Pr(n; \lambda, R) + \kappa]
  & =
    \log P^{P}(n;\lambda) +\\
  & + \frac{1}{P^{P}(n;\lambda)} (\E\check \Pr(n; \lambda, R) -
    P^{P}(n;\lambda) + \kappa) -
  \\
  & - \frac{1}{2 P^{P}(n;\lambda)^{2}} \E [\check \Pr(n; \lambda, R) -
    P^{P}(n;\lambda) + \kappa]^{2} +
    \\
  & + \frac{1}{3 P^{P}(n;\lambda)^{3}} \E [\check \Pr(n; \lambda, R) -
    P^{P}(n;\lambda) + \kappa]^{3} -
    \dots
  \end{split}
\end{equation}
If $R$ is large, $\E\check \Pr(n; \lambda, R) \approx
P^{P}(n;\lambda)$ and we need few terms only.  Note that the second
term on the right hand side is 0 by the binomial
property~\eqref{eq:EcheckPr}, and hence the first approximation is
\begin{equation}
  \label{eq:Elog_taylor_first_approx}
  \begin{split}
    \E [ \log \check \Pr(n; \lambda, R) + \kappa]
    & \approx
    \log P^{P}(n;\lambda) - 
    \frac{1}{2 P^{P}(n;\lambda)^{2}} \E [\check \Pr(n; \lambda, R) -
    P^{P}(n;\lambda) + \kappa]^{2}
    \\
    &= 
    \log P^{P}(n;\lambda) - 
    \frac{1}{2 P^{P}(n;\lambda)^{2}}[ \var \check \Pr(n; \lambda, R) + \kappa^{2}]
    \\
    & \approx 
    \log P^{P}(n;\lambda) - 
    \frac{1}{2 P^{P}(n;\lambda)^{2}} 
    \frac{P^{P}(n;\lambda) [1 - P^{P}(n;\lambda)]}{R}
    \\
    & \equiv T_{2}(n;\lambda)
  \end{split}
\end{equation}
where we ignore the small term $\kappa^{2}$ on the last line.  The
second approximation is
\begin{multline}
  \label{eq:Elog_taylor_second_approx}
    \E [ \log \check \Pr(n; \lambda, R) + \kappa] \approx 
    \\
    \approx T_{2}(n;\lambda) 
    +
    \frac{1}{3 P^{P}(n;\lambda)^{3}} \E [\check \Pr(n; \lambda, R) -
    P^{P}(n;\lambda) + \kappa]^{3}
    \\
    = T_{2}(n;\lambda) +
    \frac{1}{3 P^{P}(n;\lambda)^{3}} 
    \frac{P^{P}(n;\lambda) [1 - P^{P}(n;\lambda)] [1 - 2P^{P}(n;\lambda)]}{R^{2}}
    \\
    \equiv
    T_{3}(n;\lambda)
\end{multline}
where the third line follows from the third central moment of binomial distribution.

How good is the Taylor-series based approximation?  Look at a single
observation, \Sexpr{n <- 4} $n=4$, case:

<<ElogTaylor10, cache.extra=file.info("simulated_maximum_likelihood.R")>>=
Rs <- c(10, 100)
kappa <- 1e-9
O <- 1000
lambdas <- seq(from=1, to=10, length=100)
sll <- taylorLikelihood(n, lambdas, Rs, O=O, kappa)
yl <- range(sll$trueLogL, sll$simLogL, max(sll$T2), min(sll$T5))
sll %<>%
   tidyr::gather(key="type", value="LL", c("simLogL", "trueLogL"), matches("T[[:digit:]]")) %>%
   mutate(R = factor(R, levels=c(Rs)))
ggplot(sll %>% filter(R == Rs[1]), aes(lambda, LL)) +
   geom_line(aes(col=type)) +
   geom_vline(xintercept=mean(n), linetype="dotted") + 
   coord_cartesian(ylim=yl) +
   labs(title=paste("R =", Rs[1]))
@ 
In case of $R=10$, it appears that approximations are better than the true
likelihood (1-term approximation).  However, this is only true
sufficiently close to the maximum.  Further off, the errors become
increasingly large for higher approximations.

<<ElogTaylor100>>=
ggplot(sll %>% filter(R == Rs[2]), aes(lambda, LL)) +
   geom_line(aes(col=type)) +
   geom_vline(xintercept=mean(n), linetype="dotted") + 
   coord_cartesian(ylim=yl) +
   labs(title=paste("R =", Rs[2]))
@ 
Increasing $R=100$ makes the picture much better.  Now the simulated,
approximated, and true curve are close to each other and it is hard to
see which one is better.

If instead of using a single value of $n$ we use a vector of $n$-s,
the picture for low $R$ get noticeably worse.  Choose $n =
(\Sexpr{set.seed(11); n <- rpois(10, 5); n})$:

<<ElogTaylorMultiN, cache.extra=file.info("simulated_maximum_likelihood.R")>>=
sll <- taylorLikelihood(n, lambdas, Rs=10, O=O, kappa)
yl <- range(sll$trueLogL, sll$simLogL, max(sll$T2), min(sll$T5), max(sll$T6))
sll %<>%
   tidyr::gather(key="type", value="LL", c("simLogL", "trueLogL"), matches("T[[:digit:]]")) %>%
   mutate(R = factor(R, levels=c(Rs)))
ggplot(sll %>% filter(R == Rs[1]), aes(lambda, LL)) +
   geom_line(aes(col=type)) +
   geom_vline(xintercept=mean(n), linetype="dotted") + 
   coord_cartesian(ylim=yl) +
   labs(title=paste("R =", Rs[1]))
@ 
Clearly, the more higher terms we are adding, the worse the
approximation gets.  This is probably related to very different values
of $n$ (0 and 9); regardless of $\lambda$ value, one of these is far
off the well-approximated range of the Taylor's formula.  Try sequence
of more similar elements $n =
(\Sexpr{set.seed(11); n <- c(4, 3, 5, 4, 3, 6, 3, 4, 6, 3); n})$:

<<ElogTaylorSimilarN, cache.extra=file.info("simulated_maximum_likelihood.R")>>=
sll <- taylorLikelihood(n, lambdas, Rs=10, O=O, kappa)
yl <- range(sll$trueLogL, sll$simLogL, max(sll$T2), min(sll$T3), min(sll$T5), max(sll$T6))
sll %<>%
   tidyr::gather(key="type", value="LL", c("simLogL", "trueLogL"), matches("T[[:digit:]]")) %>%
   mutate(R = factor(R, levels=c(Rs)))
ggplot(sll %>% filter(R == Rs[1]), aes(lambda, LL)) +
   geom_line(aes(col=type)) +
   geom_vline(xintercept=mean(n), linetype="dotted") + 
   coord_cartesian(ylim=yl) +
   labs(title=paste("R =", Rs[1]))
@ 


\section{Data}
\label{sec:data}

We use Poisson distribution with true parameter value $5$.  

\section{Analyze the Bias as $N\to\infty$}
\label{sec:bias}

<<poisData>>=
N <- 10
Rs <- c(3,10,30,300,1000)
eps <- 1e-9
                           # to avid -Inf in logarithm
@ 

SML estimator has a bias term proportional to $\sqrt{N}/R$.  Let's
analyze how do the curves look like for $R = \{\Sexpr{Rs}\}$ and $N =
\{10, 10,000, 10,000,000\}$.

Start with 10 observations.  Let's simulate the estimator on grid with
different $R$:

<<poisGrid, depends="poisData">>=
N <- 10
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=150)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title= paste0("N=", N),
        subtitle=paste0("true value ", mean(x))
        )
@ 

No obvious bias is visible for low-$R$ estimators.  We can see,
though, that anything less than $R=300$ is very noisy.

Let's increase the sample size to 10000 and repeat:

<<poisLargeData, depends="poisData">>=
N <- 10000
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=150)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 

The true values is \Sexpr{mean(x)}.  We can see downward bias for the
smallest $R$, otherwise it is more upward than downward bias.  The
curves are equally noisy.

Now let's do the experiment with 10M observations:

<<pois1e7, depends="poisData">>=
N <- 1e7
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=150)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 
I don't see any major difference between 10M and 10k observations.
The bias is there for small $R$ but it appears to be of comparable
size. 


\subsection{Analyze the Chatter}
\label{sec:chatter}

If $R$ is fixed, the SLL curves are not continuous even if
$N\to\infty$.  Start with $N=10$:

<<poisFixedSeed, depends="poisData">>=
N <- 10
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=250)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      set.seed(5)
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 
The results are clearly less noisy, but only piecewise continuous.
All estimates seem to have downward bias.  Repeat it with $n=10000$:

<<poisFixedSeed10k, depends="poisData">>=
N <- 10000
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=250)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      set.seed(5)
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 
The curves are noticeably smoother, and downward bias is easily
visible for lower $R$-s.  Again, the curves are only piecewise
continuous.  I fail to see a substantial improvement over the case
with chatter above, although the curves are smoother.


10M observations:

<<poisFixedSeed10M, depends="poisData">>=
N <- 1e7
x <- rpois(N,5)
lambdas <- seq(from=1, to=10, length=250)
sll <- foreach(R = Rs, .combine=cbind) %do% {
   foreach(lambda = lambdas, .combine = c) %dopar% {
      set.seed(5)
      xx <- rpois(R, lambda)
      sPr <- tabulate(1 + xx, nbins=max(x) + 1)/R
                           # simulated probability values
      sum(log(sPr[1 + x] + eps))
   }
}
sll %<>%
   as.data.frame() %>%
   set_names(Rs) %>%
   cbind("Inf" = sapply(lambdas, function(lambda) sum(dpois(x, lambda, log=TRUE)))) %>%
   cbind(lambda=lambdas) %>%
   tidyr::gather(key="R", value="SLL", matches("^([[:digit:]]+|Inf)$")) %>%
   mutate(R = factor(R, levels=c(Rs, "Inf")))
ggplot(sll, aes(lambda, SLL)) +
   geom_line(aes(col=R)) +
   geom_smooth(aes(col=R), se=FALSE) +
   geom_vline(xintercept=mean(x), linetype="dotted") +
   labs(title=paste0("N=", N),
        subtitle = paste0("Sample mean ", mean(x))
        )
@ 

\end{document}
